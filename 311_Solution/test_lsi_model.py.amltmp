import pandas as pd
from datetime import datetime
from test_LSI_model_functions import trim_test_set, test_LSI_model, write_threshold_csv

class lsi_model_req(object):
    def __init__(self, dictionary, tfidf_matrix, lsi_base_corpus):
        self.dictionary = dictionary
        self.tfidf_matrix = tfidf_matrix
        self.lsi_base_corpus = lsi_base_corpus

def status_message(imsg):
    s = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')
    s = s[:-4]
    timestamp = '[' + s + ']'
    print(timestamp + ' ' + str(imsg))





########################################################################################
# Read in Sentences
status_message("Reading in Sentences")
df = pd.read_csv('../../John.Yawney/transcription_csv/phrase_options_df.csv')
df = df[df['phraseIndex'] == 0]
df['lexical'] = df['lexical'].fillna('')

## SET PARAMETERS
## Get Sentences that match query
test_type = 'individual-blob' # all-blobs, individual-blob, subset-blobs, raw-text
model_name = 'LSImodel_sentence_topic'
query = 'thank you for calling the city'
topic = 'greeting'
conf_thresh = 0.6
test_df = df

# Test on Single Blob
if test_type = 'individual-blob':
    test_subset_ind = True
    blob_name = '28e54f69-90a1-4efb-bc0d-4b7893af2dea/audio-adastra-processing/2506321_UCMAVoice_2020102817050870.mp3.json'
    test_df = df[df['blobName'] == blob_name]

# Trim
test_group_blob_ind = True
if test_group_blob_ind:
    test_subset_ind = True
    # Trim sentences
    min_perc = 0
    max_perc = 0.1
    trimmed_df = trim_test_set(df, min_perc, max_perc)
    # DF to Use
    test_df = trimmed_df


# Get LSI Result
thresh_list = test_LSI_model(
    query, 
    model_name, 
    conf_thresh = conf_thresh, 
    test_subset_ind = test_subset_ind,
    test_df = test_df
    )

# Gather Phrases from LSI Result
thresh_list_df = pd.DataFrame(thresh_list, columns = ["index", "corr_conf"])
thresh_list_df['phrase'] = thresh_list_df['index'].apply(lambda x: test_df.iloc[x]['lexical'])
thresh_list_df = thresh_list_df.drop_duplicates(subset=['phrase'])

# Write Threshold
write_threshold_csv(thresh_list_df, topic)


# Gather Sentences
# df['phraseIndexByBlob'] = df.groupby(['blobName']).cumcount() + 1
# df[df['phraseIndexByBlob'] > 1][df['phraseIndexByBlob'] < 5]['lexical'].to_csv('./luis_sentences/not_greeting.csv', index=False)
